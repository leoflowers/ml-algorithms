{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension: multi-class logistic regression\n",
    "# base algorithm: binary logistic regression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity of ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  alcohol  malic acid   ash  alcalinity of ash  magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   total phenols  flavanoids  nonflavanoid phenols  proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   color intensity   hue  OD280/OD315 of diluted wines  proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data reading and preprocessing\n",
    "# since we are dealing with categorical features, we translate them to integer labels.\n",
    "# we can do this without headaches since we are not running any actual regression on our data.\n",
    "\n",
    "df = pd.read_csv('../datasets/wine.csv', header=None, names = [\"label\", \"alcohol\",\"malic acid\",\"ash\", \"alcalinity of ash\", \n",
    "                                                   \"magnesium\", \"total phenols\", \"flavanoids\",\n",
    "                                                   \"nonflavanoid phenols\", \"proanthocyanins\", \"color intensity\",\n",
    "                                                   \"hue\", \"OD280/OD315 of diluted wines\", \"proline\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"alcohol\",\"malic acid\",\"ash\", \"alcalinity of ash\", \n",
    "         \"magnesium\", \"total phenols\", \"flavanoids\",\n",
    "         \"nonflavanoid phenols\", \"proanthocyanins\", \"color intensity\",\n",
    "         \"hue\", \"OD280/OD315 of diluted wines\", \"proline\"]\n",
    "\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df[names],df['label'], random_state=2435234)\n",
    "\n",
    "\n",
    "X_train=df_X_train.to_numpy()\n",
    "X_test=df_X_test.to_numpy()\n",
    "y_train=df_y_train.to_numpy()\n",
    "y_test=df_y_test.to_numpy()\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# prepending training samples with 1\n",
    "ones = np.ones(X_train.shape[0]).reshape((X_train.shape[0], 1))\n",
    "X_train = np.hstack((ones, X_train))\n",
    "\n",
    "\n",
    "# prepending test samples with 1\n",
    "ones = np.ones(X_test.shape[0]).reshape((X_test.shape[0], 1))\n",
    "X_test = np.hstack((ones, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function for extension\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(np.array(z))\n",
    "    return e_z / np.sum(e_z, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing parameters w_i for extension; denoted W\n",
    "\n",
    "features = X_train.shape[1]\n",
    "classes = 3                                # three classes according to dataset\n",
    "\n",
    "W = np.zeros((classes, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis function for multiclass\n",
    "\n",
    "def multi_hypothesis(X, W):\n",
    "    if X.ndim == 1:\n",
    "        return softmax(np.matmul(W, X))\n",
    "    else:\n",
    "        result = []\n",
    "        for i in range(0, X.shape[0]):\n",
    "            result.append(softmax(np.matmul(W, X[i])))\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical cross_entropy\n",
    "\n",
    "def cat_x_entropy(X , y , W):\n",
    "    log_likelihood = 0\n",
    "    h = multi_hypothesis(X, W)\n",
    "    \n",
    "    for i in range(0, X.shape[0]):\n",
    "        val = -1 * np.log(h[i][y[i]-1])\n",
    "        log_likelihood += val\n",
    "    return -1*log_likelihood / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, classes):\n",
    "    vec = np.zeros(classes)\n",
    "    vec[label - 1] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified gradient ascent\n",
    "\n",
    "features = X_train.shape[1]\n",
    "classes = 3 \n",
    "\n",
    "def multi_gradient_ascent(X, y, learning_rate, num_iters):\n",
    "    entropy_values = []\n",
    "    W = np.zeros((classes, features))\n",
    "    N = X.shape[0] \n",
    "  \n",
    "    for i in range(0, num_iters):\n",
    "        h_current = multi_hypothesis(X, W)\n",
    "        for j in range(0, W.shape[0]):\n",
    "            gradient_term = np.zeros((W.shape[1], ))\n",
    "            for l in range(0, X.shape[0]):\n",
    "                y_n = one_hot_encode(y[l], classes)\n",
    "                gradient_term += np.multiply(y_n[j] - h_current[l][j], X[l])                \n",
    "            W[j] += (learning_rate/N)*gradient_term \n",
    "            \n",
    "        if (i % 100) == 0:\n",
    "            entropy_values.append([i, cat_x_entropy(X,y,W)])\n",
    "        \n",
    "    return W, entropy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37582874  1.34856126  0.33368576  0.94976528 -1.61363153  0.11834935\n",
      "   0.55401524  1.0777676  -0.11753371 -0.06018848  0.09566429  0.12027834\n",
      "   0.99276151  1.70810654]\n",
      " [ 0.47480413 -2.02325426 -0.57554064 -1.74324623  1.37723292 -0.27134544\n",
      "  -0.47182437  0.55048141  0.25580044  0.6303248  -1.61297529  1.26050208\n",
      "   0.29143598 -2.23124107]\n",
      " [-0.85063287  0.674693    0.24185488  0.79348095  0.23639861  0.15299609\n",
      "  -0.08219087 -1.62824901 -0.13826674 -0.57013632  1.517311   -1.38078042\n",
      "  -1.28419749  0.52313453]]\n"
     ]
    }
   ],
   "source": [
    "# fitting our model\n",
    "\n",
    "learning_rate = 0.25\n",
    "num_iters = 2000      # The number of iterations to run the gradient ascent algorithm\n",
    "\n",
    "W, entropy_values = multi_gradient_ascent(X_train, y_train, learning_rate, num_iters)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my accuracy:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# measuring accuracy\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    hyp = multi_hypothesis(X_test[i], W)\n",
    "    if np.argmax(hyp) == y_test[i]-1:\n",
    "        correct += 1\n",
    "\n",
    "print(\"my accuracy: \", correct / X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit accuracy:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# measuring accuracy of scikit's implementation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(\"scikit accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1483939d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*zip(*entropy_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
